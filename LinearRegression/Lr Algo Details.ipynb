{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Analysis\n",
    "\n",
    "In all business case scnearios , we come up with two very important terminologies.\n",
    "\n",
    "1. **Independent Variable**\n",
    "2. **Dependent Variable**\n",
    "\n",
    "As an example we can think of a company holding their annual budget meeting , and out of every other cost factor that will be decided in the meeting , one of the most important decision that needs to be taken is the salary structure of the employees. \n",
    "\n",
    "If we think logically then can identify areas which might affect the salary of an individual.\n",
    "\n",
    "1. **Performence**\n",
    "2. **Adehrenece**\n",
    "3. **Efficinecy**\n",
    "4. **Previous Pay**\n",
    "\n",
    "All the above factors described can be called as the independent variables in this case scenario. Now let's remember why we are taking all these independent variables into consideration.. To determine the salary of an individual. So the salary is dependent here on all the above mentioned variables. Hence the dependent variable in this case scenario would be -\n",
    "\n",
    "1. **Salary**\n",
    "\n",
    "The Analysis that is conducted to establish a relation between this dependent and independent variables is called a \"Regression Analysis\".\n",
    "\n",
    "Remember that in regression analysis the dependent variable will always be of continuos and numeric in nature\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "\n",
    "In case of linear regression , the relationship between dependent and independent variable is linear in nature. Now when we talk about the linear nature , it simply means then when the value of independent variable increases , the value of dependent variable increase as well ,and vice versa. There are two types of linear regression.\n",
    "\n",
    "1. **Simple Linear Regression** - Where the Number of independent variable is one\n",
    "2. **Multiple Linear Regression** - Where the number of independent variable is more than one\n",
    "\n",
    "Please remember that in both the scenarios the number of dependent variable remains one only."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression Co-efficients\n",
    "\n",
    "Co efficients are the element which determines that which independent variable is more important to determine the value of dependent variable.\n",
    "\n",
    "Lets's take a house price prediction into consideration , the independent variables are represneted with x and dependent variable is represneted here with y\n",
    "\n",
    "**Independent Variables**\n",
    "\n",
    "1. Location - x1\n",
    "2. Area by sq ft - x2\n",
    "3. Previous owner's name - x3\n",
    "4. Crime rate in locality - x4\n",
    "\n",
    "**Dependent variable**\n",
    "\n",
    "1. House price - y\n",
    "\n",
    "Now judging by the above example it can clearly be stated that only independent variable which is not at all important when it comes to determine the House price is 'Previous owner's name', so it's co efficient value will be less than others. let's suppose the co efficient values are ranging from 1 to 4 as we have 4 independent variables here ,1 being the lowest and 4 being the highest , so now as per their respective co efficient value we can re order the independent variables as below ,\n",
    "\n",
    "1. Location - x1(3 - coef value)\n",
    "2. Area by sq ft - x2(4 - coef value)\n",
    "3. Previous owner's name - x3(1 - coef value)\n",
    "4. Crime rate in locality - x4(2 - coef value)\n",
    "\n",
    "The determining of the co efficient values for all independent variables will be done by training our model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intercept\n",
    "\n",
    "Now we need to understand a bit about graphs and x,y axis . If we are analyzing a dataset then we need to plot the data points into different axis of x and y. Let's say x and y axis both has 2 different data point such as 4,1 ... now if we plot these data points , the line we are plotting , it will intersect both x and y axis somewhere. The point where it is intersecting the x axis will be called as x intercept and where it is intersecting with y axis , it will be called as y intercept.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slope\n",
    "\n",
    "Slope as a definition is something that defined the rate of change in y axis in respect to x axis.It simply means if the value of x is changing then according to that change , on what rate the value of y is also changing.\n",
    "\n",
    "Slope = change in y/change in x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent & Cost Function\n",
    "\n",
    "##### MSE (Mean Squared Error - Cost Function)\n",
    "Let's take an example of house price prediction. Suppose our data set has 6 data points , that has \n",
    "\n",
    "1. **House Area**\n",
    "2. **House Price**\n",
    "\n",
    "Now ofcourse our house price is the dependent/target variable here. When we will build a linear regression model then our model will draw a linear line to align all the avaiable data points. Now based on the data multiple such linear lines are drawn in the model. The line here contains all the predicted values and the data already have the actual value in it. Now the difference between each predcited data point and acttual data point are called error. \n",
    "\n",
    "1. Our first job is to perform a square operation in all the errors recieved from above.\n",
    "2. The square function is used because our error value might be a positive or negative integer but we need to make all the values in positive only. \n",
    "3. Then we will perform a summation of all the errors\n",
    "4. Then the summation will be divided by the number of datasets , 6 for this case.\n",
    "5. For whichever line the error rate is the lowest ,it'll be considered as the best fit line.\n",
    "\n",
    "##### Gradient Descent\n",
    "\n",
    "Gradient descent is an algorithm that finds best fit line for given training data set. This algorithm or gradient search will continue to iterate through our training data points and based on our cost function (MSE in this case) will start finding the cost (error) , with all iteration it will reach the lowest error value and will plot the best fit line accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "github1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2f1f49a464b0261529b7f879af28a2fa50ede0bedb86dbf326bb328bc03475b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
